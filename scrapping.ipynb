{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADGSENPAI\\AppData\\Local\\Temp\\ipykernel_23944\\401634970.py:41: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  broswer = webdriver.Chrome(\n"
     ]
    }
   ],
   "source": [
    "from email.mime import image\n",
    "import json\n",
    "import profile\n",
    "from re import template\n",
    "from turtle import Shape\n",
    "import bs4 as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import yaml\n",
    "import requests\n",
    "import urllib\n",
    "import os\n",
    "import cairosvg\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "# open config.yaml\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# create images folder if not exists\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "\n",
    "\n",
    "url = config['linkedin_profile']['url']\n",
    "\n",
    "profileinfo = {\n",
    "    'phone': config['info']['phone'],\n",
    "    'email': config['info']['email'],\n",
    "    'location': config['info']['location'],\n",
    "    'discord': config['info']['discord'],\n",
    "    'instant': config['info']['instant'],\n",
    "    'website': config['info']['website'],\n",
    "    'headline': config['info']['headline'],\n",
    "}\n",
    "\n",
    "themes = config['themes'][0]['path']\n",
    "\n",
    "broswer = webdriver.Chrome(\n",
    "    executable_path='./chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "broswer.get(url)\n",
    "\n",
    "\n",
    "# get all html code from the page\n",
    "html = broswer.page_source\n",
    "\n",
    "broswer.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ADGSENPAI\\COde\\CVGeneratorFromLinkedin\\scrapping.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADGSENPAI/COde/CVGeneratorFromLinkedin/scrapping.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m soup \u001b[39m=\u001b[39m bs\u001b[39m.\u001b[39mBeautifulSoup(html, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADGSENPAI/COde/CVGeneratorFromLinkedin/scrapping.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# get the name of the person\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADGSENPAI/COde/CVGeneratorFromLinkedin/scrapping.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m name \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mfind(\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ADGSENPAI/COde/CVGeneratorFromLinkedin/scrapping.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mh1\u001b[39;49m\u001b[39m'\u001b[39;49m, class_\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtop-card-layout__title font-sans text-lg papabear:text-xl font-bold leading-open text-color-text mb-0\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADGSENPAI/COde/CVGeneratorFromLinkedin/scrapping.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m about \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADGSENPAI/COde/CVGeneratorFromLinkedin/scrapping.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcore-section-container__content break-words\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADGSENPAI/COde/CVGeneratorFromLinkedin/scrapping.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# find all li profile-section-card  experience-item\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "soup = bs.BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# get the name of the person\n",
    "name = soup.find(\n",
    "    'h1', class_='top-card-layout__title font-sans text-lg papabear:text-xl font-bold leading-open text-color-text mb-0').text.strip()\n",
    "about = soup.find(\n",
    "    'div', class_='core-section-container__content break-words').text.strip()\n",
    "\n",
    "# find all li profile-section-card  experience-item\n",
    "experience__list = soup.find_all('ul', class_='experience__list')\n",
    "experience__list = soup.find_all(\n",
    "    'li', class_='profile-section-card experience-item')\n",
    "\n",
    "jobs = []\n",
    "\n",
    "for job in experience__list:\n",
    "    currentjob = {}\n",
    "    currentjob['title'] = job.find(\n",
    "        'h3', class_='profile-section-card__title').text.strip()\n",
    "    currentjob['subtitle'] = job.find(\n",
    "        'h4', class_='profile-section-card__subtitle').text.strip()\n",
    "    range = job.find(\n",
    "        'p', class_='experience-item__duration experience-item__meta-item').text.strip().split(' ')\n",
    "    index = range[3].find('Present')\n",
    "    if index != -1:\n",
    "        number = range[3].replace('Present', '')\n",
    "        range[3] = 'Present ' + number\n",
    "    else:\n",
    "        # remove first 4 characters from string\n",
    "        year = range[4][0:4]\n",
    "        number = range[4][4:]\n",
    "        range[4] = year + ' ' + number\n",
    "    # join the list\n",
    "    currentjob['range'] = ' '.join(range)\n",
    "    currentjob['location'] = job.find(\n",
    "        'p', class_='experience-item__location experience-item__meta-item').text.strip()\n",
    "    if (job.find('p', class_='show-more-less-text__text--more')) != None:\n",
    "        currentjob['description'] = job.find(\n",
    "            'p', class_='show-more-less-text__text--more').text.strip()\n",
    "    else:\n",
    "        currentjob['description'] = job.find(\n",
    "            'p', class_='show-more-less-text__text--less').text.strip()\n",
    "    currentjob['image'] = job.find('img')['data-delayed-url']\n",
    "    jobs.append(currentjob)\n",
    "\n",
    "edus = []\n",
    "\n",
    "educations = soup.find_all(\n",
    "    'li', class_='profile-section-card education__list-item')\n",
    "for education in educations:\n",
    "    edu = {}\n",
    "    edu['school'] = education.find(\n",
    "        'h3', class_='profile-section-card__title').text.strip()\n",
    "    degrees = education.find_all(\n",
    "        'span', class_='education__item education__item--degree-info')\n",
    "    degreetext = ''\n",
    "    for degree in degrees:\n",
    "        degreetext += degree.text.strip() + ' '\n",
    "    edu['degree'] = degreetext\n",
    "\n",
    "    edu['range'] = education.find(\n",
    "        'p', class_='education__item education__item--duration').text.strip()\n",
    "    if education.find('p', class_='education__item education__item--activities-and-societies') != None:\n",
    "        edu['activities'] = education.find(\n",
    "            'p', class_='education__item education__item--activities-and-societies').text.strip()\n",
    "\n",
    "    try:\n",
    "        edu['description'] = education.find(\n",
    "            'div', class_='show-more-less-text').text.strip()\n",
    "    except:\n",
    "        edu['description'] = ''\n",
    "\n",
    "    edu['image'] = education.find('img')['data-delayed-url']\n",
    "\n",
    "    edus.append(edu)\n",
    "\n",
    "certs = []\n",
    "\n",
    "certifications = soup.find_all('ul', class_='certifications__list')[0]\n",
    "certifications = certifications.find_all('li', class_='profile-section-card')\n",
    "\n",
    "for certification in certifications:\n",
    "    cert = {}\n",
    "    cert['title'] = certification.find(\n",
    "        'h3', class_='profile-section-card__title').text.strip()\n",
    "    cert['image'] = certification.find('img')['data-delayed-url']\n",
    "    cert['subtitle'] = certification.find(\n",
    "        'h4', class_='profile-section-card__subtitle').text.strip()\n",
    "    cert['range'] = certification.find(\n",
    "        'span', class_='certifications__start-date').text.strip()\n",
    "    try:\n",
    "        cert['credentialid'] = certification.find(\n",
    "            'div', class_='certifications__credential-id').text.strip()\n",
    "    except:\n",
    "        cert['credentialid'] = ''\n",
    "    try:\n",
    "        cert['link'] = certification.find(\n",
    "            'a', class_='certifications__button')['href']\n",
    "    except:\n",
    "        cert['link'] = ''\n",
    "\n",
    "    certs.append(cert)\n",
    "\n",
    "pubs = []\n",
    "publications = soup.find_all('ul', class_='publications__list')[0]\n",
    "\n",
    "publications = publications.find_all(\n",
    "    'li', class_='profile-section-card personal-project')\n",
    "\n",
    "for publication in publications:\n",
    "    pub = {}\n",
    "    pub['title'] = publication.find(\n",
    "        'h3', class_='profile-section-card__title').text.strip()\n",
    "    pub['subtitle'] = publication.find(\n",
    "        'h4', class_='profile-section-card__subtitle').text.strip().replace('\\n', '')\n",
    "    # remove unnecessary spaces with a line\n",
    "    pub['subtitle'] = ' '.join(pub['subtitle'].split())\n",
    "\n",
    "    try:\n",
    "        pub['description'] = publication.find(\n",
    "            'p', class_='show-more-less-text__text--more').text.strip()\n",
    "    except:\n",
    "        try:\n",
    "            pub['description'] = publication.find(\n",
    "                'p', class_='show-more-less-text__text--less').text.strip()\n",
    "        except:\n",
    "            pub['description'] = ''\n",
    "\n",
    "    try:\n",
    "        pub['link'] = publication.find(\n",
    "            'a', class_='personal-project__button')['href']\n",
    "    except:\n",
    "        pub['link'] = ''\n",
    "    pubs.append(pub)\n",
    "\n",
    "\n",
    "import qrcode\n",
    "img = qrcode.make(profileinfo['website'])\n",
    "img.save('images/qrcode.png')\n",
    "\n",
    "\n",
    "\n",
    "# save the data in a json file\n",
    "data = {\n",
    "    'name': name,\n",
    "    'about': about,\n",
    "    'jobs': jobs,\n",
    "    'education': edus,\n",
    "    'certifications': certs,\n",
    "    'publications': pubs,\n",
    "    'profile_info': profileinfo,\n",
    "    'linkedin_url': url,\n",
    "    'qrcode': 'images/qrcode.png',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save all images in images folder with extension as downloaded\n",
    "\n",
    "global imageindex\n",
    "imageindex = 0\n",
    "def downloadImages(arr):    \n",
    "    global imageindex\n",
    "    for a in arr:\n",
    "        try:            \n",
    "            imageurl = a['image']\n",
    "            #download image\n",
    "            image = requests.get(imageurl) \n",
    "            # check if image is a jpeg or svg\n",
    "            if image.headers['Content-Type'] == 'image/jpeg':\n",
    "                with open('images/' + str(imageindex) + '.jpg', 'wb') as f:\n",
    "                    f.write(image.content)                \n",
    "                    a['image'] = 'images/' + str(imageindex) + '.jpg'\n",
    "            else:                \n",
    "                with open('images/' + str(imageindex) + '.svg', 'wb') as f:\n",
    "                    f.write(image.content)\n",
    "                #convert svg using cairosvg\n",
    "                cairosvg.svg2png(url='images/' + str(imageindex) + '.svg', write_to='images/' + str(imageindex) + '.png')\n",
    "                a['image'] = 'images/' + str(imageindex) + '.png'\n",
    "                                                                      \n",
    "            imageindex += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "profilephoto = soup.find('div', class_='top-card__profile-image-container top-card-layout__entity-image-container flex top-card__profile-image-container--cvw-fix')\n",
    "profilephoto = profilephoto.find('img')['src']\n",
    "profilephoto = requests.get(profilephoto)\n",
    "with open('images/profilephoto.jpg', 'wb') as f:\n",
    "    f.write(profilephoto.content)\n",
    "\n",
    "# Open the input image as numpy array, convert to RGB\n",
    "img=Image.open(\"images/profilephoto.jpg\").convert(\"RGB\")\n",
    "npImage=np.array(img)\n",
    "h,w=img.size\n",
    "\n",
    "# Create same size alpha layer with circle\n",
    "alpha = Image.new('L', img.size,0)\n",
    "draw = ImageDraw.Draw(alpha)\n",
    "draw.pieslice([0,0,h,w],0,360,fill=255)\n",
    "\n",
    "# Convert alpha Image to numpy array\n",
    "npAlpha=np.array(alpha)\n",
    "\n",
    "# Add alpha layer to RGB\n",
    "npImage=np.dstack((npImage,npAlpha))\n",
    "\n",
    "# Save with alpha\n",
    "Image.fromarray(npImage).save('images/profilephoto.png')\n",
    "\n",
    "data['profile_photo'] = 'images/profilephoto.png'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "downloadImages(jobs)\n",
    "downloadImages(edus)\n",
    "downloadImages(certs)\n",
    "downloadImages(pubs)\n",
    "\n",
    "        \n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "  \n",
    "\n",
    "\n",
    "from docx.shared import Cm\n",
    "from docxtpl import DocxTemplate, InlineImage\n",
    "from docx.shared import Cm, Inches, Mm, Emu\n",
    "\n",
    "\n",
    "\n",
    "template = DocxTemplate(themes)\n",
    "\n",
    "data['qrcode'] = InlineImage(template, 'images/qrcode.png', width=Mm(30.27))\n",
    "\n",
    "# set all InlineImages\n",
    "def setInlineImages(arr):\n",
    "    for a in arr:\n",
    "        try:\n",
    "            a['image'] = InlineImage(template, a['image'], width=Mm(15.19))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "setInlineImages(jobs)\n",
    "setInlineImages(edus)\n",
    "setInlineImages(certs)\n",
    "setInlineImages(pubs)\n",
    "\n",
    "#set profile photo as InlineImage circle\n",
    "data['profile_photo'] = InlineImage(template, 'images/profilephoto.png', width=Mm(25.82), height=Mm(25.82))\n",
    "\n",
    "template.render(data)\n",
    "\n",
    "template.save('cv.docx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
